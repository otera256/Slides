# 深層学習の基礎と自動微分

- 目標: MLPを構成している要素技術を理解する

## 前回の復習

線形層と活性化関数の組み合わせで非線形関数を近似できる（Universal Approximation Theorem）

一般に一層ごとの次数を大きくするよりも多層に分けた方が少ないパラメータで近似できる

誤差逆伝搬法で勾配を計算し、勾配降下法でパラメータを更新することで学習できる

## 今回の内容

- 計算グラフと自動微分の仕組み
- 初期値の重要性
- Optimizerの工夫
- 正規化手法の紹介

## 計算グラフと自動微分

- 計算グラフ: 数式をノードとエッジで表現したもの
- 順伝搬: 入力から出力まで計算グラフをたどって計算を行うこと
- 逆伝搬: 出力から入力まで計算グラフをたどって勾配を計算すること

スライドでは計算グラフの実例を示していくつかの関数で考え方の説明を行ってから、本題のMLPを導入する

- Pythonでのオブジェクト指向を利用した実装例の紹介
- 各ノードでの局所的な勾配計算と連鎖律の適用による全体の勾配計算の仕組みの説明
- 自動微分の利点: 手動で微分を計算する必要がなく、複雑なモデルでも効率的に勾配を計算できる
- 自動微分の欠点: メモリ使用量が増えることや、計算グラフの構築に時間がかかること。実装ではメモリリークに注意が必要
- PyTorchでは動的グラフ,TensorFlowでは静的グラフが使われていることの説明

## 初期値の重要性

- ニューラルネットワークの学習において、パラメータの初期値は学習の収束速度や最終的な性能に大きな影響を与える（前回の実装でデタラメに大きい値を入れた場合の挙動を確認）
- まず順伝搬でactivationの分散がどのように変化するかを考える
- 次に逆伝搬で勾配の分散がどのように変化するかを考える
- 分散が大きくなりすぎたり小さくなりすぎたりすると、勾配消失や勾配爆発が起こりやすくなる
- 分散が変化しないように初期値を設定する方法として、Xavier初期化やHe初期化がある
- すこし踏み込んで行列の初期値に一様分布を使う場合とガウス分布を使う場合で分布がどのように変化するかを厳密に考察する
- 分散だけの議論では不十分となりうることを説明する（第4.5回への布石）
- 実際の実装例の紹介

## Optimizerの工夫

スライドでは等高線グラフ上での図も使用してわかりやすくする
適切なYouTube動画も紹介する

- SGDの問題点: 学習率の調整が難しい、局所最適解に陥りやすい
- モーメンタム法: 過去の勾配を利用して更新方向を滑らかにする
- AdaGrad: 各パラメータごとに学習率を調整する
- RMSProp: AdaGradの改良版で、勾配の二乗平均を利用して学習率を調整する
- Adam: モーメンタム法とRMSPropを組み合わせた手法で、現在最も広く使われている
- 各Optimizerの特徴と使い分けのポイントの説明
- 実装面ではパラメータごとにオプティマイザーの状態を管理する必要があることを説明
- PyTorchでの実装例の紹介
- ハイパーパラメータのチューニングの重要性と一般的な手法の紹介

## 正規化手法の紹介

過学習の問題点: 訓練データに対しては高い精度を示すが、未知のデータに対しては性能が低下する（train errorとtest errorの違いを図・グラフで示す）
- ドロップアウト: 学習時にランダムにニューロンを無効化することで過学習を防ぐ手法
- バッチ正規化: 各ミニバッチごとにデータを正規化することで学習を安定化させる手法
- レイヤー正規化: バッチ正規化の代替手法で、各サンプルごとに正規化を行う
- 重み減衰（L2正則化）: 損失関数にパラメータの二乗和を加えることで過学習を防ぐ手法（詳細はL1正則化と含めて第4.5回で扱う）
- 各正規化手法の特徴と使い分けのポイントの説明
- 実装面での注意点の説明
- PyTorchでの実装例の紹介

## 機械学習フレームワークの構造

- 実装の参考となるように、PyTorchやTensorFlowなどの機械学習フレームワークがどのように構成されているかを簡単に説明
- 計算グラフの構築と自動微分の仕組み
- モデルの定義とパラメータ管理
- データセットからのデータ読み込みと前処理
- Optimizerと正規化手法の実装
- フレームワークの拡張性とカスタマイズ方法の紹介
- 実際のフレームワークのコード例の紹介
- まとめと今後の学習へのアドバイス